#!/bin/bash

# Function to display usage information
usage() {
  echo "Usage: $0 <domain_list_file>"
  echo "       $0 -h  Display this help message"
  exit 1
}

# Check if help is requested or no file name is provided
if [ "$#" -ne 1 ]; then
  usage
elif [ "$1" = "-h" ]; then
  usage
fi

# Check if the file exists
if [ ! -f "$1" ]; then
  echo "File not found: $1"
  exit 1
fi

# Suppose you have a relative path
relative_path="$1"

# Convert to absolute path
path_file=$(
  cd "$(dirname "$relative_path")"
  pwd
)/$(basename "$relative_path")

# Absolute directory where the Scrapy project is located
SCRAPE_DIR="./lastPart/urls_js_extractor"
OUT_DIR="./urls_endpoints"

cd "$SCRAPE_DIR"
# Loop through each domain in the file using a subshell to avoid changing the current directory permanently
while IFS= read -r domain; do
  if [[ -n "$domain" ]]; then # Check if the line is not empty
    output_file="$OUT_DIR/${domain//[^a-zA-Z0-9]/_}.json"
    # Run the Scrapy spider for each domain and output to a domain-specific file
    echo " Scraping $domain :"
    scrapy crawl urls_extractor -a domain="$domain" -o "$output_file" >../../logs/scrapy_url_extractor.txt 2>&1

    # # Check if the file is empty or contains only an empty array and delete if it does
    # if [ -f "$output_file" ] && (! -s "$output_file" || grep -qE '^\s*\[\s*\]\s*$' "$output_file"); then
    #   rm "$output_file"
    #   echo "No endpoints found for $domain, no file created."
    # else
    # Additional Scrapy run for js_extractor spider using the existing output file
    echo " Extracting JS files for $domain :"
    scrapy crawl js_extractor -a url_file="$output_file" -o "../JsFiles/${domain}_js.json" >../../logs/scrapy_js_extractor.txt 2>&1
    # fi
  fi
done <"$path_file"
