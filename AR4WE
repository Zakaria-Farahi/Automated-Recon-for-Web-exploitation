#!/bin/bash

# Author: Zakaria Farahi & Saad Lili & Haraoui Souhaib
# Created: 10 May 2024
# Last modified: 10 May 2024

# Description: This script is used to do reconaissance on a target domain. It uses various tools to gather information about the target domain.

# Usage: AR4WE [options] <domain>
usage() {
  echo "Usage: $0 <domain>"
  echo "  domain: The domain for which to enumerate subdomains."
  exit 1
}
# Check for help flag or incorrect number of arguments
if [ "$1" = "-h" ] || [ $# -ne 1 ]; then
  usage
fi
# Vars
in_domain=$1
current_date=$(date +"%Y-%m-%d_%H-%M-%S")
nmap_out_dir="./port_Scan/nmap/$current_date"
sublist3r_out_dir="./Subdomain/Sublist3r/$current_date"
# sublist3r_out_dir="Subdomain/Sublist3r/2024-05-11_00-38-32"

# Run Sublist3r ----------------------------------------------
echo "Running Sublist3r for the domain: $in_domain"
mkdir -p $sublist3r_out_dir
sublist3r -d $in_domain -o "$sublist3r_out_dir/res_Sub.txt" >/dev/null 2>&1

# Run nmap ---------------------------------------------------------------
mkdir -p "$nmap_out_dir"
# Loop through each line in the file
while IFS= read -r domain; do
  if [[ -n "$domain" ]]; then # Ensure the line is not empty
    echo "Running nmap for $domain..."
    # Customize your nmap scan options here; for example, -sV attempts to determine service/version info
    nmap -sV "$domain" -oN "$nmap_out_dir/${domain//[^a-zA-Z0-9]/_}.txt" >/dev/null 2>&1
  fi
done <"$sublist3r_out_dir/res_Sub.txt"

# run urls_js_extractor ------------------------------------------------------------
SCRAPE_DIR="./lastPart/urls_js_extractor"
OUT_DIR="./urls_endpoints"
cd "$SCRAPE_DIR"

# Loop through each domain in the file using a subshell to avoid changing the current directory permanently
while IFS= read -r domain; do
  if [[ -n "$domain" ]]; then # Check if the line is not empty
    output_file="$OUT_DIR/$current_date/${domain//[^a-zA-Z0-9]/_}.json"
    # Run the Scrapy spider for each domain and output to a domain-specific file
    echo "Scraping $domain :"
    scrapy crawl urls_extractor -a domain="$domain" -o "$output_file" >../../logs/scrapy_url_extractor.txt 2>&1
    echo "Extracting JS files for $domain :"
    scrapy crawl js_extractor -a url_file="$output_file" -o "../JsFiles/$current_date/${domain}_js.json" >../../logs/scrapy_js_extractor.txt 2>&1
  fi
done <"../../$sublist3r_out_dir/res_Sub.txt"
cd ../../
