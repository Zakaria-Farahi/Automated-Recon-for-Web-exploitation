import scrapy

class DomainSpider(scrapy.Spider):
    name = 'urls_extractor'

    def start_requests(self):
        # Accept domain from command line argument
        url = getattr(self, 'domain', None)
        if url:
            url = 'http://' + url if not url.startswith('http') else url
            yield scrapy.Request(url, self.parse)
        else:
            self.logger.error("Domain not provided. Use the -a option to specify the domain.")
            raise scrapy.exceptions.CloseSpider(reason="Domain not provided")

    def parse(self, response):
        # Extract all links on the page
        for href in response.css('a::attr(href)'):
            full_url = response.urljoin(href.extract())
            yield {
                'URL': full_url
            }
            # Optionally follow the link (uncomment to enable)
            # yield response.follow(href, callback=self.parse)

        # Example: Extracting specific endpoints that contain 'api'
        if 'api' in response.url:
            yield {
                'API Endpoint': response.url
            }
