import scrapy
import json

class JsExtractorSpider(scrapy.Spider):
    name = 'js_extractor'

    def __init__(self, url_file=None, *args, **kwargs):
        super(JsExtractorSpider, self).__init__(*args, **kwargs)
        if url_file:
            with open(url_file, 'r') as file:
                data = json.load(file)
                self.start_urls = [item['URL'] for item in data]

    def parse(self, response):
        # Extract JavaScript file links
        js_links = response.css('script::attr(src)').getall()
        for js in js_links:
            if js:  # Ensure it's not empty
                full_js_url = response.urljoin(js)
                yield {'URL': full_js_url}
        # Optionally, follow all hyperlinks on the page to find more JavaScript files
        # Uncomment the following lines if you want to crawl deeper
        # for href in response.css('a::attr(href)').getall():
        #     yield response.follow(href, self.parse)
