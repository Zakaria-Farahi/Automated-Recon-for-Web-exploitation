#!bin/bash

if ! scrapy --version >/dev/null 2>&1; then
    echo "Scrapy is not installed. installing it ..."
    pip install scrapy
fi
# Define the project and spider names
PROJECT_NAME="url_extractor"
SPIDER_NAME="domain_spider"
URLS_EXTRACTOR_FILE="$SPIDER_NAME.py"
JS_EXTRACTOR_FILE="js_extractor.py"

scrapy startproject $PROJECT_NAME

# Path to the spiders directory
SPIDERS_DIR="$PROJECT_NAME/$PROJECT_NAME/spiders"

# Create the spider file and write the spider code to it
cat <<EOF >$SPIDERS_DIR/$SPIDER_FILE
import scrapy

class DomainSpider(scrapy.Spider):
    name = 'domain_spider'

    def start_requests(self):
        # Accept domain from command line argument
        url = getattr(self, 'domain', None)
        if url:
            url = 'http://' + url if not url.startswith('http') else url
            yield scrapy.Request(url, self.parse)
        else:
            self.logger.error("Domain not provided. Use the -a option to specify the domain.")
            raise scrapy.exceptions.CloseSpider(reason="Domain not provided")

    def parse(self, response):
        # Extract all links on the page
        for href in response.css('a::attr(href)'):
            full_url = response.urljoin(href.extract())
            yield {
                'URL': full_url
            }
            # Optionally follow the link (uncomment to enable)
            # yield response.follow(href, callback=self.parse)

        # Example: Extracting specific endpoints that contain 'api'
        if 'api' in response.url:
            yield {
                'API Endpoint': response.url
            }
EOF
# Create the spider file and write the spider code to it
cat <<EOF >$SPIDERS_DIR/$JS_EXTRACTOR_FILE
import scrapy

class DomainSpider(scrapy.Spider):
    name = 'domain_spider'

    def start_requests(self):
        # Accept domain from command line argument
        url = getattr(self, 'domain', None)
        if url:
            url = 'http://' + url if not url.startswith('http') else url
            yield scrapy.Request(url, self.parse)
        else:
            self.logger.error("Domain not provided. Use the -a option to specify the domain.")
            raise scrapy.exceptions.CloseSpider(reason="Domain not provided")

    def parse(self, response):
        # Extract all links on the page
        for href in response.css('a::attr(href)'):
            full_url = response.urljoin(href.extract())
            yield {
                'URL': full_url
            }
            # Optionally follow the link (uncomment to enable)
            # yield response.follow(href, callback=self.parse)

        # Example: Extracting specific endpoints that contain 'api'
        if 'api' in response.url:
            yield {
                'API Endpoint': response.url
            }
EOF
