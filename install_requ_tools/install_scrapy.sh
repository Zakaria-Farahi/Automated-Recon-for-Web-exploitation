#!bin/bash

# Check if the script is run as root
if [ "$(id -u)" != "0" ]; then
    echo "This script must be run as root."
    exit 1
fi

if ! scrapy --version >/dev/null 2>&1; then
    echo "Scrapy is not installed. installing it ..."
    pip install scrapy
fi

# Define the project and spider names
PROJECT_NAME="urls_js_extractor"
URLS_EXTRACTOR_FILE="urls_extractor.py"
JS_EXTRACTOR_FILE="js_extractor.py"

# PROJECT_PATH=$(pwd)/../$PROJECT_NAME
PROJECT_PATH="../lastPart/$PROJECT_NAME"

# Check if the directory exists
if [ -d "$PROJECT_PATH" ]; then
    echo "The directory $PROJECT_PATH already exists."
    exit 1
else
    sudo scrapy startproject $PROJECT_NAME
    # Check if the project directory exists and move it if it does
    if [ -d "$PROJECT_NAME" ]; then
        sudo mv "$PROJECT_NAME" $PROJECT_PATH
    else
        echo "Failed to create project"
        exit 1
    fi
fi

# Path to the spiders directory
SPIDERS_DIR="$PROJECT_PATH/$PROJECT_NAME/spiders"

# Create the spider file and write the spider code to it
cat <<EOF >$SPIDERS_DIR/$URLS_EXTRACTOR_FILE
import scrapy

class DomainSpider(scrapy.Spider):
    name = 'urls_extractor'

    def start_requests(self):
        # Accept domain from command line argument
        url = getattr(self, 'domain', None)
        if url:
            url = 'http://' + url if not url.startswith('http') else url
            yield scrapy.Request(url, self.parse)
        else:
            self.logger.error("Domain not provided. Use the -a option to specify the domain.")
            raise scrapy.exceptions.CloseSpider(reason="Domain not provided")

    def parse(self, response):
        # Extract all links on the page
        for href in response.css('a::attr(href)'):
            full_url = response.urljoin(href.extract())
            yield {
                'URL': full_url
            }
            # Optionally follow the link (uncomment to enable)
            # yield response.follow(href, callback=self.parse)

        # Example: Extracting specific endpoints that contain 'api'
        if 'api' in response.url:
            yield {
                'API Endpoint': response.url
            }
EOF
# Create the spider file and write the spider code to it
cat <<EOF >$SPIDERS_DIR/$JS_EXTRACTOR_FILE
import scrapy
import json

class JsExtractorSpider(scrapy.Spider):
    name = 'js_extractor'

    def __init__(self, url_file=None, *args, **kwargs):
        super(JsExtractorSpider, self).__init__(*args, **kwargs)
        if url_file:
            with open(url_file, 'r') as file:
                data = json.load(file)
                self.start_urls = [item['URL'] for item in data]

    def parse(self, response):
        # Extract JavaScript file links
        js_links = response.css('script::attr(src)').getall()
        for js in js_links:
            if js:  # Ensure it's not empty
                full_js_url = response.urljoin(js)
                yield {'URL': full_js_url}
        # Optionally, follow all hyperlinks on the page to find more JavaScript files
        # Uncomment the following lines if you want to crawl deeper
        # for href in response.css('a::attr(href)').getall():
        #     yield response.follow(href, self.parse)
EOF
